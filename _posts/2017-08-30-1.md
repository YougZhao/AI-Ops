---
layout:         post
title:          支持向量机(Support Vector Machine, SVM)
subtitle:       
card-image:     /assets/images/2017-08-30-1_0.jpg
author:         Yang ZHAO
date:           2017-08-30 01:00:00
tags:           机器学习
post-card-type: image
---

对于支持向量机来说，数据点被视为p维向量，而我们想知道是否可以用(p-1)维超平面来分开这些点。这就是所谓的线性分类器。

>SVM的分类适用性可以说是现在所有分类器中比较好的了，对于线性分类问题，SVM可以使用线性核；对于多项式分类，LDA可以使用多项式核；对于比较零散的数据分类，我们可以使用RBF核，而且SVM不仅仅只有这些核函数，也可以自己创造核函数，来达到最好的效果。我们并不能通过理论来确定高维的向量数据具体用哪种核比较好，只能通过试验，交叉验证等方法找最合适的核，或者在实践中编写出一个比较理想核模型。SVM分类的不适用性是在超大数据量时，由于有两个参数gamma和C，这两个参数牵扯到比较多的数据计算（SVM的惩罚机制），会导致整体数据计算效率的下降，所以SVM比较适合与小型或者中型的数据量。

我们先在二维平面举例说明，在二维平面上分割数据的就是一维的直线。支持向量机能对线性可分的类别进行分类，并找到距离两类最接近超平面的点，使他们到超平面的距离相同并且最大。如下图中，和都是能够线性分割两类的超平面，但是我们可以看到，比的分类效果更好，因为它使得两类里最接近该直线的点到直线的距离相同并且最大。

![pic](/assets/images/2017-08-30-1_1.png)

在支持向量机中，我们有的时候会碰到线性不可分的情况，这时候我们就需要用到核技巧，核技巧就是将数据进行低维到高维的转换。在支持向量机中，核技巧有三种：线性、Radial Basis Function，和多项式。这三种方法对于相同数据集会得到不同数据分类结果，如下图：

![pic](/assets/images/2017-08-30-1_2.png)

可能比较难理解的就是RBF这种核技巧了，这种技巧是根据离已有数据点的远近来判别位置数据点的分类的，有点像下图高斯分布的形式。那个黑色的点就是我们的已有数据点，离它越近，那么就越容易得到与该点相同分类。

![pic](/assets/images/2017-08-30-1_3.png)

对于支持向量机，训练样本可以是多维（可以为一维、两维、三维……）特征空间向量，其中每个训练样本带有一个类别标签。算法的训练阶段只包含存储的特征向量和训练样本的标签。并且需要知道有几类数据，才能进行训练，并不能自动聚类。注意，此处的与并不是代表平面中的坐标。

比如在实际运用中，我们需要将数据准备好并划分好他们对应的标签，例如我们将可以报警转换成多维的向量，并将报警分为两类，给他们打好标签。在此处，如何设计转换成的向量要依据实际情况而定。最后得到的向量分布是不需要在空间中尽量线性可分的，因为我们可以之后看分类情况来决定用各种不同的核技巧，并调整不同的参数。

在训练数据阶段，我们可能需要调整两个主要参数，一个是C，一个是gamma。对于参数C，所有的SVM核基本都是一样的，我们在此处需要做一个权衡。当C的值较低时，我们可以得到相对光滑的分类表面，然而当C的值较高时，我们可以得到较高的准确率。Gamma值决定了一个点的影响力有多大，如果gamma值越大，那么越近的点越会受到影响。

下面两张图，当我们控制gamma时，左边当C为1，右边当C为10000，结果可以米线看到右边的分类更细腻，同时结果并没有左边光滑。

![pic](/assets/images/2017-08-30-1_4.png)
![pic](/assets/images/2017-08-30-1_5.png)

下面两张图，当我们控制C时，左边当gamma为0.5，右边当gamma为1000，结果可以明显看到右边受影响的点很近。

![pic](/assets/images/2017-08-30-1_6.png)
![pic](/assets/images/2017-08-30-1_7.png)

接下来是一个算法的简单实现实例，我们将整个算法一步步分析，并且了解它是如何运行的，这里主要描述一个大概思路，程序在ipython notebook下已测试。

加载需要的library

{% highlight python %}
import numpy as np
import matplotlib.pyplot as plt  
from sklearn import svm, datasets  
%pylab inline  
from sklearn.neighbors import KNeighborsClassifier
{% endhighlight %}

加载iris数据集

{% highlight python %}
# import some data to play with  
iris = datasets.load_iris()  
X = iris.data[:, :2] # we only take the first two features. We could  
# avoid this ugly slicing by using a two-dim dataset  
y = iris.target
{% endhighlight %}

为之后画出分布图所做的准备

{% highlight python %}
# create a mesh to plot in  
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1  
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1  
h = (x_max / x_min)/100  
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))
{% endhighlight %}

将数据通过三种不同的核进行训练，此处我们只保留了RBF核。

{% highlight python %}
# we create an instance of SVM and fit out data. We do not scale our  
# data since we want to plot the support vectors  
C = 1.0 # SVM regularization parameter  
#svc = svm.SVC(kernel='linear', C=1,gamma='auto').fit(X, y)  
svc = svm.SVC(kernel='rbf', C=1,gamma=0.5).fit(X, y)  
#svc = svm.SVC(kernel='poly', C=1,gamma='auto').fit(X, y)  
knn = KNeighborsClassifier(1) #classifier  
%timeit knn.fit(X,y) 
{% endhighlight %}

将结果的分布图打印出来。

{% highlight python %}
plt.subplot(1, 1, 1)  
Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])  
Z = Z.reshape(xx.shape)  
plt.contourf(xx, yy, Z, alpha=0.3)  
plt.scatter(X[:, 0], X[:, 1], c=y)  
plt.xlabel('Sepal length')  
plt.ylabel('Sepal width')  
plt.xlim(xx.min(), xx.max())  
plt.title('SVC with RBF kernel')  
plt.show()
{% endhighlight %}

打出的图片即为之前的图片样式。